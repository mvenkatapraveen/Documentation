This documentation provides a quick reference for basic commands used in **Sqoop**, **Hive**, and **PySpark**. These tools are essential for data processing and integration in the Hadoop ecosystem.

## 1. Sqoop

### What is Sqoop?
**Sqoop** is a tool designed for efficiently transferring bulk data between relational databases and Apache Hadoop.

**_1.1. [Sqoop Basics](./sqoop/basics.md)_**

## 2. HIVE

### What is Hive?
**Hive** is a data warehouse software built on top of Hadoop that allows querying and managing large datasets using a SQL-like language called HiveQL.

**_2.1. [Hive Basics](./hive/basics.md)_**

## 3. PySpark

### What is PySpark?
PySpark is the Python API for Apache Spark, which allows data processing using distributed computing and is a popular tool for big data processing.

**_3.1. [PySpark Basics using RDD and DF](./pyspark/basics.md)_**
**_2.1. [Scenarios](./pyspark/scenarios.md)_**
