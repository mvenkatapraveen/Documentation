This documentation provides a quick reference for basic commands used in **Sqoop**, **Hive**, and **PySpark**. These tools are essential for data processing and integration in the Hadoop ecosystem.

## 1. Sqoop

### What is Sqoop?
**Sqoop** is a tool designed for efficiently transferring bulk data between relational databases and Apache Hadoop.

**_[Sqoop Basics](./sqoop/basics.md)_**

## 2. HIVE

### What is Hive?
**Hive** is a data warehouse software built on top of Hadoop that allows querying and managing large datasets using a SQL-like language called HiveQL.

**_[Hive Basics](./hive/basics.md)_**

## 3. PySpark

### What is PySpark?
PySpark is the Python API for Apache Spark, which allows data processing using distributed computing and is a popular tool for big data processing.

**_[PySpark Basics using RDD and DF](./pyspark/basics.md)_**
<br>
**_[Basic Scenarios](./pyspark/scenarios.md)_**
<br>
**_[Interview Scenarios](./pyspark/interview_scenarios.md)_**
