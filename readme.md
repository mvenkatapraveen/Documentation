This documentation provides a quick reference for Hadoop Introduction and important concepts in **Sqoop**, **Hive**, and **PySpark**. These tools are essential for data processing and integration in the Hadoop ecosystem.

## 1. Hadoop

### What is Hadoop?
Hadoop is an open-source framework used for distributed storage and processing of large sets of data across clusters of computers.

**_[1.1 Introduction](./hadoop/intro.md)_**

## 2. Sqoop

### What is Sqoop?
**Sqoop** is a tool designed for efficiently transferring bulk data between relational databases and Apache Hadoop.

**_[2.1 Sqoop Commands](./sqoop/commands.md)_**

## 3. HIVE

### What is Hive?
**Hive** is a data warehouse software built on top of Hadoop that allows querying and managing large datasets using a SQL-like language called HiveQL.

**_[3.1 Hive Basics](./hive/basics.md)_**

## 4. PySpark

### What is PySpark?
PySpark is the Python API for Apache Spark, which allows data processing using distributed computing and is a popular tool for big data processing.

**_[4.1 Introduction]()_**
<br>
**_[4.2 RDD Basics](./pyspark/commands_rdd.md)_**
<br>
**_[4.3 Data Frames processing using SQL & DSL](./pyspark/commands_sql_dsl.md)_**
<br>
**_[4.4 Basic Scenarios](./pyspark/scenarios.md)_**
<br>
**_[4.5 Interview Scenarios](./pyspark/interview_scenarios.md)_**
