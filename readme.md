This documentation provides a quick reference for basic commands used in **Sqoop**, **Hive**, and **PySpark**. These tools are essential for data processing and integration in the Hadoop ecosystem.

## 1. Sqoop

### What is Sqoop?
**Sqoop** is a tool designed for efficiently transferring bulk data between relational databases and Apache Hadoop.

**_[1.1 Sqoop Commands](./sqoop/commands.md)_**

## 2. HIVE

### What is Hive?
**Hive** is a data warehouse software built on top of Hadoop that allows querying and managing large datasets using a SQL-like language called HiveQL.

**_[2.1 Hive Basics](./hive/basics.md)_**

## 3. PySpark

### What is PySpark?
PySpark is the Python API for Apache Spark, which allows data processing using distributed computing and is a popular tool for big data processing.

**_[3.1 RDD Basics](./pyspark/commands_rdd.md)_**
<br>
**_[3.2 Data Frames processing using SQL & DSL](./pyspark/commands_sql_dsl.md)_**
<br>
**_[3.3 Basic Scenarios](./pyspark/scenarios.md)_**
<br>
**_[3.4 Interview Scenarios](./pyspark/interview_scenarios.md)_**
